---
title: "News Article Summarization with Transformers"
excerpt: "Implemented a transformer-based encoder-decoder model from scratch in PyTorch for abstractive text summarization
of news articles. <br/><br/><img src='/images/xsum.webp'>"
collection: Portfolio
---

Full code repository available on <a href="https://github.com/florianfrick/News-Summarization">Github</a>. <br><br>

I wrote the positional embedding, attention head, encoder, and decoder blocks from scratch in PyTorch. Then, used the Bart Tokenizer and trained on the XSum dataset of BBC articles 
and their associated single-sentence summaries.

<br/><br/><img src='/images/xsum.webp'><br><br>

My results were promising and highlight some of the obstacles faced in real world natural language processing problems.
Thus, I wrote the associated <a href="https://medium.com/correll-lab/building-an-encoder-decoder-for-text-summarization-c66ddf23f466">Medium</a> article to provide the
community with a useful resource to learn and implement an encoder-decoder architecture.