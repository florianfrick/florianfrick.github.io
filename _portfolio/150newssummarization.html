---
title: "News Article Summarization with Transformers"
excerpt: "Implemented a transformer-based encoder-decoder model from scratch in PyTorch for abstractive text summarization
of news articles. <br/><br/><img src='/images/xsum.webp'>"
collection: Portfolio
---

Full code repository available on <a href="https://github.com/florianfrick/News-Summarization">Github</a>. <br><br>

I wrote the positional embedding, attention head, encoder, and decoder blocks from scratch in PyTorch. Then, used the Bart Tokenizer and trained on the XSum dataset of BBC articles 
and their associated single-sentence summaries.

<br/><br/><img src='/images/xsum.webp'><br><br>

While the results were promising, the low ROGUE Score ultimately demonstrates how difficult natural language processing from scratch is.
To actually solve real world problems, its best to start with a model pretrained on a huge corpus to understand language and simply finetune it for the desired task.
<br><br>
Nonetheless, the encoder-decoder transformer architecture is incredibly powerful, so I wrote an associated <a href="https://medium.com/correll-lab/building-an-encoder-decoder-for-text-summarization-c66ddf23f466">Medium</a> article to provide the
community with a useful resource to implement the model themselves in PyTorch.